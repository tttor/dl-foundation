# optim
See also:
* https://github.com/tttor/math-foundation/tree/master/optim
* https://github.com/tttor/rl-foundation/tree/master/method/policy-based/optim

# Newton-type methods
* Natural Gradient: KFAC
* LBFGS
* HFO: Hessian-Free Opt
* Krylov Subspace Descent, Oriol Vinyals et al
  * not require the approximation of the Hessian to be PSD, however, it requires more memory.

# Misc
* https://www.quora.com/Why-second-order-optimization-method-impractical-for-training-neural-network
* https://www.quora.com/Why-are-optimization-techniques-like-natural-gradient-and-second-order-methods-L-BFGS-for-eg-not-much-used-in-deep-learning
* https://stats.stackexchange.com/questions/253632/why-is-newtons-method-not-widely-used-in-machine-learning
* https://scicomp.stackexchange.com/questions/14513/minimisation-problem-in-thousands-of-dimensions

## book, course
* https://mitpress.mit.edu/books/optimization-machine-learning
* http://www.cs.cornell.edu/courses/cs6787/2017fa/

## tutor
* http://andrew.gibiansky.com/blog/machine-learning/gauss-newton-matrix/
* http://andrew.gibiansky.com/blog/machine-learning/hessian-free-optimization/
* http://andrew.gibiansky.com/blog/machine-learning/conjugate-gradient
* https://justindomke.wordpress.com/2009/01/17/hessian-vector-products/
* http://ruder.io/deep-learning-optimization-2017/
* http://ruder.io/optimizing-gradient-descent/index.html
* https://studywolf.wordpress.com/2016/04/04/deep-learning-for-control-using-augmented-hessian-free-optimization/
  * https://github.com/studywolf/blog/blob/master/train_AHF/train_hf.py

