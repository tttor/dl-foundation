# Grad descent
* Adaptive learning rate methods
  * Adagrad
  * RMSProp: identical to Adagrad, but the cache variable is a “leaky”
  * Adam: like RMSProp with momentum.
